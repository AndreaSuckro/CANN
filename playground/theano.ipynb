{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano example implementation for data classification\n",
    "\n",
    "This example should illustrate the functionality of Theano on the TensorFlow examples. It is structured similar to the other demos. The first block is concerned with data generation and the latter with two implementations in Theano: a single perceptron and a multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing all the things\n",
    "%matplotlib notebook\n",
    "\n",
    "import theano\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import linalg as la\n",
    "from numpy.random import randn, randint\n",
    "from playground import viz\n",
    "from playground import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = DataGenerator()\n",
    "\n",
    "fig = plt.figure('Available Datasets')\n",
    "for i, name in enumerate(['circle', 'xor', 'gauss', 'spiral']):\n",
    "    data, labels = getattr(datasets, name)(200)\n",
    "    axis = plt.subplot(221+i)\n",
    "    axis.set_title(name)\n",
    "    axis.scatter(*zip(*data), c=labels, cmap='bwr')\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following setting helps with debugging and proved to be rather useful - although you need to learn the error messages and what they are supposed to say as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debug settings (not that it would help a lot...)\n",
    "theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first implementation here shows a simple perceptron implementation in Theano. Take a look at the code and play around with the parameters for the number of training steps or change the loss function and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## LINEAR REGRESSION\n",
    "\n",
    "N = 400 # number of samples\n",
    "feats = 20 # dimensionality of features\n",
    "D = (randn(N, feats), randint(size=N, low=0, high=2)) # rand data\n",
    "training_steps = 10000\n",
    "\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "w = theano.shared(randn(20), name=\"w\")\n",
    "b = theano.shared(0., name=\"b\")\n",
    "\n",
    "print(\"-------- Perceptron Model --------\")\n",
    "print(w.get_value(), b.get_value())\n",
    "\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w)-b)) # probability that target = 1\n",
    "prediction = p_1 > 0.5 # the prediction threshold\n",
    "xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1) # cross-entropy loss func\n",
    "cost = xent.mean() + 0.01 * (w**2).sum() # the cost to minimize\n",
    "gw, gb = T.grad(cost, [w, b])\n",
    "\n",
    "train = theano.function(inputs = [x, y], outputs = [prediction, xent],\n",
    "                        updates = {w : w-0.1*gw, b : b-0.1*gb})\n",
    "\n",
    "predict = theano.function(inputs = [x], outputs = prediction)\n",
    "\n",
    "\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "    \n",
    "print(\"-------- Training result -------\")\n",
    "print(w.get_value(), b.get_value())\n",
    "print(\"Target values for the data: \")\n",
    "print(D[1])\n",
    "print(\"Predictions on the data: \")\n",
    "print(predict(D[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend it to the MLP with a hidden layer. You may try to add further layers or change again the parameters of the problem. If you increase the epochs take also care of the debuging print statements (otherwise your computation slows down and the notebook can get stuck).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MULTILAYER-PERCEPTRON\n",
    "\n",
    "# Choose your problem!\n",
    "# datasets.circle(), datasets.xor(), datasets.gauss(), datasets.spiral()\n",
    "n = 1000\n",
    "data, labels = datasets.xor(n)\n",
    "D = (data, labels.flatten())\n",
    "\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "w_1 = theano.shared(rnd.randn(2,10), name=\"w1\")\n",
    "b_1 = theano.shared(np.zeros((10,)), name=\"b1\")\n",
    "w_2 = theano.shared(rnd.randn(10), name=\"w2\")\n",
    "b_2 = theano.shared(0., name=\"b2\")\n",
    "\n",
    "print(\"-------- Multilayer Model --------\")\n",
    "print(w_1.get_value(), b_1.get_value())\n",
    "print(w_2.get_value(), b_2.get_value())\n",
    "\n",
    "# expression graph for forward propagation\n",
    "out = T.nnet.sigmoid(-T.dot(T.nnet.sigmoid(-T.dot(x,w_1)-b_1), w_2)-b_2)\n",
    "\n",
    "prediction = out > 0.5\n",
    "\n",
    "# cross entropy as loss function\n",
    "cross_ent = -y * T.log(out) - (1-y) * T.log(1-out)\n",
    "# cost function\n",
    "cost = cross_ent.mean() + 0.01 * (w_2**2).sum()\n",
    "gw_1, gb_1, gw_2, gb_2 = T.grad(cost, [w_1, b_1, w_2, b_2])\n",
    "\n",
    "# compile the model\n",
    "train = theano.function(inputs = [x, y], outputs = [prediction, cross_ent],\n",
    "                        updates = [(w_1, w_1-0.1*gw_1), (b_1, b_1-0.1*gb_1),\n",
    "                                   (w_2, w_2-0.1*gw_2), (b_2, b_2-0.1*gb_2)])\n",
    "\n",
    "predict = theano.function([x], prediction)\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "#fig = plt.figure('Intermediate Results')\n",
    "for epoch in range(epochs):\n",
    "    # get batches of training data\n",
    "    b_indx = rnd.choice(n, batch_size)\n",
    "    b_data = D[0][b_indx]\n",
    "    b_target = D[1][b_indx]\n",
    "    pred, err = train(D[0], D[1])\n",
    "    \n",
    "    if (epoch+1)%100 == 0:\n",
    "        perf = 1 - sum(abs(predict(D[0]) - D[1])) / n\n",
    "        #viz(D[0], predict(D[0], (epoch+1)/epochs, perf, fig, discretize=False))\n",
    "        print(\"Training... {:3.2%} done with accuracy {:2.2f}\".format((i+1)/epochs, perf))\n",
    "    \n",
    "\n",
    "print(\"-------- Training result -------\")\n",
    "print(w_1.get_value(), b_1.get_value())\n",
    "print(w_2.get_value(), b_2.get_value())\n",
    "\n",
    "print(\"Target values for the data: \")\n",
    "print(D[1])\n",
    "print(\"Predictions on the data: \")\n",
    "print(predict(D[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
