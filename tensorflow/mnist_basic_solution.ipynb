{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow MNIST - Basic - Solution\n",
    "The MNIST database (Mixed National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. For some backgrund on the data, see [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "This notebook will guide you through implementing a basic TensorFlow feed forward neural network to classify handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Each image consists of $28 \\times 28$ pixels. We will treat each pixel as a feature, therefore we have 784 individual features. In the first cell below we will first download the data, that might take a second but only has to be done once. In the next cell we then visualize some samples from the data so you have an idea what you are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "fig = plt.figure('Available Datasets')\n",
    "for i, sample in enumerate(choice(len(mnist.train.images), 9, replace=False)):\n",
    "    image = mnist.train.images[sample].reshape((28,28))\n",
    "    label = mnist.train.labels[sample]\n",
    "    axis = plt.subplot(331+i)\n",
    "    axis.set_title(np.argmax(label))\n",
    "    axis.imshow(image, cmap='gray')\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Each of the data samples consists of 784 features and there is a total of 10 digits. Remember that in TensorFlow the placeholders need to reflect that we want to handle the data in batches -- therefore the input and output are not one but two dimensional. The calculations for the output predictions are already given here. Feel free to try out different activation functions found in the TensorFlow [API Documentation](https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#neural-network). For details on why we use softmax here checkout the [official tutorial](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html#softmax-regressions).\n",
    "\n",
    "For learning we are going to use a gradient descent optimizer already provided by TensorFlow. A well proven error measure is [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy):\n",
    "\n",
    "$$-\\sum(targets * log(predictions))$$\n",
    "\n",
    "TensorFlow provides functions for all of those calculations: `tf.reduce_sum` and `tf.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None,784), name='input')\n",
    "t = tf.placeholder(tf.float32, shape=(None, 10), name='target')\n",
    "\n",
    "W = tf.Variable(tf.zeros((784,10)), name='weights')\n",
    "b = tf.Variable(tf.zeros((10)), name='biases')\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b, name='predictions')\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(t*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing\n",
    "THe training and testing code again is given. Take note of the correct prediction checking and accuracy measurement: The target vectors always contain nine zeros and a single one. Using `tf.argmax` we retrieve the index of the single one from the predictions and from the targets and compare them with each other. In the accuracy measure we then cast this boolean result to a float (results in either `0.` or `1.` and calculate the mean over the whole batch using `tf.reduce_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(t,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_x, batch_t = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, {x: batch_x, t: batch_t})\n",
    "\n",
    "# Check the results.\n",
    "print(sess.run(accuracy, {x: mnist.test.images, t: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
