{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Playground\n",
    "Welcome to the TensorFlow Playground! This notebook is modeled after the official [playground.tensorflow.org](http://playground.tensorflow.org) web application which offers an playful way to test different feed forward neural network configurations. This notebook gives you the opportunity to actually implement different network configurations you might already have tested in the web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperations\n",
    "You can basically ignore this first code cell below. All it does is generate the four different datasets and provide a visualization function. What you can do here is adjust N, the amount of samples we generate. If you feel fancy you can also tweak the datasets to make the problems harder by for example adding some noise to the spiral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import normal, random_sample, permutation\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def viz(data, labels, progress, accuracy):\n",
    "    \"\"\"Function visualizing training progress.\"\"\"\n",
    "    if not hasattr(fig, 'viz'): viz.fig = plt.figure('Result')\n",
    "    viz.fig.suptitle(\"{:3.2%} done with accuracy {:2.2f}\"\n",
    "                     .format((epoch+1)/epochs, accuracy))\n",
    "    plt.scatter(*zip(*data), c=labels, cmap='bwr')\n",
    "    viz.fig.canvas.draw()\n",
    "\n",
    "\n",
    "class Datasets:\n",
    "\n",
    "    # Returns n floats uniformly distributed between a and b.\n",
    "    uniform = lambda self, a, b, n=1: (b - a) * random_sample((1, n)) + a\n",
    "    \n",
    "    def splitsets(self, data, targets, split=10):\n",
    "        \"\"\"Splits samples into train and test sets and reshapes target labels as required.\"\"\"\n",
    "        n = data.shape[0]\n",
    "        permut = permutation(n).astype(int)\n",
    "        targets = np.array([targets, np.abs(targets-1)]).T\n",
    "        return tuple(zip(*((d[permut[n//split:]], d[permut[:n//split]]) for d in (data, targets))))\n",
    "\n",
    "    def circle(self, n=1000, radius=5, split=10):\n",
    "        \"\"\"Generates data with one class describing an inner and one an outer circle.\"\"\"\n",
    "        radius = 5\n",
    "        r = np.append(self.uniform(0, radius*.5, n//2), self.uniform(radius*.7, radius, n//2))\n",
    "        angle = self.uniform(0, 2*np.pi, n)\n",
    "        xy = np.vstack((r * np.sin(angle), r * np.cos(angle)))\n",
    "        t = np.less(norm(xy, axis=0), radius*.5)\n",
    "        return self.splitsets(xy.T, t, split)\n",
    "    \n",
    "    def xor(self, n=1000, padding=.3, split=10):\n",
    "        \"\"\"Generates xor data.\"\"\"\n",
    "        x = self.uniform(-5, 5, N); x[x>padding] += padding; x[x<padding] -= padding\n",
    "        y = self.uniform(-5, 5, N); y[y>padding] += padding; y[y<padding] -= padding\n",
    "        t = np.less(x*y, 0).flatten()\n",
    "        return splitsets(np.vstack((x, y)).T, t, split)\n",
    "\n",
    "    def gauss(self, n=1000, split=10):\n",
    "        \"\"\"Generates two blops of gauss data, each describing a class.\"\"\"\n",
    "        return splitsets(np.vstack((normal(2, 1, (N//2, 2)), normal(-2, 1, (N//2, 2)))),\n",
    "                         np.append(np.ones(N//2), np.zeros(N//2)))\n",
    "\n",
    "    def genSpiral(self, delta, n):\n",
    "        \"\"\"Generates a spiral of n points with rotation-offset delta.\"\"\"\n",
    "        points = np.arange(n)\n",
    "        r = points / n * 5\n",
    "        t = 1.75 * points / n * 2 * np.pi + delta\n",
    "        return np.vstack((r * np.sin(t), r * np.cos(t))).T\n",
    "    \n",
    "    def spiral(self, n=1000):\n",
    "        \"\"\"Generates a dataset of two intertwined spirals.\"\"\"\n",
    "        return splitsets(np.vstack((self.genSpiral(0, n//2), genSpiral(np.pi, n//2))),\n",
    "                         np.append(np.ones(n//2), np.zeros(n//2)))\n",
    "\n",
    "data = Datasets()\n",
    "circle = data.circle()\n",
    "xor = data.xor()\n",
    "gauss = data.gauss()\n",
    "spiral = data.spiral()\n",
    "    \n",
    "# visualize\n",
    "fig = plt.figure('Available Datasets')\n",
    "fig.suptitle('only showing test set')\n",
    "ax1 = plt.subplot(221); ax1.set_title('circle')\n",
    "ax1.scatter(*zip(*circle[1][0]), c=circle[1][1][:,0], cmap='bwr')\n",
    "ax2 = plt.subplot(222); ax2.set_title('xor')\n",
    "ax2.scatter(*zip(*xor[1][0]), c=xor[1][1][:,0], cmap='bwr')\n",
    "ax3 = plt.subplot(223); ax3.set_title('gauss')\n",
    "ax3.scatter(*zip(*gauss[1][0]), c=gauss[1][1][:,0], cmap='bwr')\n",
    "ax4 = plt.subplot(224); ax4.set_title('spiral')\n",
    "ax4.scatter(*zip(*spiral[1][0]), c=spiral[1][1][:,0], cmap='bwr')\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Select your dataset here by simply replacing the variable name at the very left. Some information about the provided data structures:\n",
    "\n",
    "The two `*_data` variables contain the actual data in 2 dimensional numpy arrays, the first axis describes individual observations, the second axis the `x` and `y` values. Therefore they have a shape of `(m, 2)`. Similarilly the two `*_target` variables are arrays containing the corresponding labels, again in a shape of `(m, 2`), $samples \\times classes$: The second axis binarily describes weather a sample belongs to the first class (the first value in the second axis will equal `1`, the second value `0`) or to the second class (second value will be `1`..).\n",
    "\n",
    "As the names suggest, `train_data` and `train_targets` are designated for training and contain a lot more data (by default $9/10$ of the data is training data) and `test_data` and `test_targets` are meant for evaluating the networks performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# circle, xor, gauss, spiral\n",
    "(train_data, train_targets), (test_data, test_targets) = gauss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn!\n",
    "In the next cell stuff gets interesting. We preimplemented a basic model which is sufficiently powerful for the gauss dataset. It consists of 2 neurons in the input layer to feed `x` and `y` values into, 4 neurons in the single hidden layer and again 2 neurons in the output layer for giving probabilities for each of our two classes.\n",
    "\n",
    "Instead of using buttons and dropdowns you will need to get your hands dirty here in order to being able to cope with more complex datasets! While we provide some guidance to mock the experience of the web application it will be also super useful to checkout the [TensorFlow API documentation](https://www.tensorflow.org/versions/r0.9/api_docs/index.html). Because the documentation does not give any theoretical background on all those different functions you might also want to regularily consultate Wikipedia.\n",
    "\n",
    "  * Adjust the hidden layer, play around with more or even fewer neurons.\n",
    "  * Add another hidden layer! You will need to adjust the matrix multiplications for its surrounding layers in order to integrate it.\n",
    "  * Change the activation functions. By default both, the hidden and the output layer, use the sigmoid activation function. Similar to the web application you can use `relu` or `tanh`. You might also consider trying to implement your own `linear` activation function -- TensorFlow does not provide one by itself.\n",
    "  * Take a look into different error measures and optimizers. This is nothing you can change in the playground web application but actually a powerful tool to tweak your networks performance through better training. Checkout the [optimizers chapter](https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#optimizers) in the documentation.\n",
    "  * Lastly, take a look at the actual training loop. Tweak the amount of iterations and change the batch size. You can actually also tweak the batch size in the web app, take a look at the bottom left. For changing the ratio of training to test data and to add noise to the data you will need to hack around in the top code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2], name='input_layer')\n",
    "\n",
    "W_h = tf.Variable(tf.random_normal([2,4]), name='hidden_weights')\n",
    "b_h = tf.Variable(tf.zeros([4]), name='hidden_bias')\n",
    "h = tf.nn.sigmoid(tf.matmul(x,W_h) + b_h, name='hidden_layer')\n",
    "\n",
    "W_y = tf.Variable(tf.random_normal([4,2]), name='output_weights')\n",
    "b_y = tf.Variable(tf.zeros([2]), name='output_bias')\n",
    "y = tf.nn.sigmoid(tf.matmul(h,W_y) + b_y, name='output_layer')\n",
    "\n",
    "t = tf.placeholder(tf.float32, shape=[None, 2], name='targets')\n",
    "\n",
    "error = tf.reduce_mean(tf.squared_difference(t,y), name='mean_squared_error')\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.03).minimize(error, name='gradient_descent')\n",
    "\n",
    "prediction_check = tf.equal(tf.argmax(y,1), tf.argmax(t,1), name='prediction_check')\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32), name='accuracy_measure')\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    split = permutation(len(train_targets))\n",
    "    batch_targets = train_targets[split[:10]]\n",
    "    batch_data = train_data[split[:10]]\n",
    "    sess.run(optimizer, {x: batch_data, t: batch_targets})\n",
    "    if (epoch+1)%100 == 0:\n",
    "        perf = sess.run(accuracy, {x: test_data, t: test_targets})\n",
    "        predictions = sess.run(y, {x: test_data})\n",
    "        viz(test_data, predictions[:,0], (epoch+1)/epochs, perf)\n",
    "\n",
    "perf = sess.run(accuracy, {x: test_data, t: test_targets})\n",
    "predictions = sess.run(y, {x: test_data})\n",
    "viz(test_data, predictions[:,0], 1, perf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
