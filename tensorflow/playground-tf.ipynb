{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Playground\n",
    "Welcome to the TensorFlow Playground! This notebook is modeled after the official [playground.tensorflow.org](http://playground.tensorflow.org) web application which offers an playful way to test different feed forward neural network configurations. This notebook gives you the opportunity to actually implement different network configurations you might already have tested in the web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "As with the original web application you have the choice between four different datasets. The next cell visualizes them and in the cell right below those visualizations you can make your choice. To use anything but the gauss dataset you can simply exchange the `datasets.gauss` function call with either `datasets.circle`, `datasets.xor` or `datasets.spiral`. Additionally you can add some noise to the data to make solving the problem more difficult. Some information about the provided data structures:\n",
    "\n",
    "The two `*_data` variables contain the actual data in 2 dimensional numpy arrays, the first axis describes individual observations, the second axis the `x` and `y` values. Therefore they have a shape of `(m, 2)`. The `*_labels` variables are simply `(m,1)` numpy arrays specifying a label, `0` or `1`, for each sample.\n",
    "\n",
    "As the names suggest, `train_data` and `train_labels` are designated for training and contain a lot more data (by default $9/10$) and `test_data` and `test_labels` are meant for evaluating the networks performance. You can change the amount of samples inside the `gauss` (or whatever you choose) function call and also adjust the split between training and testing data by adjusting the second parameter for the `datasets.split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from playground import DataGenerator\n",
    "\n",
    "datasets = DataGenerator()\n",
    "\n",
    "fig = plt.figure('Available Datasets')\n",
    "for i, name in enumerate(['circle', 'xor', 'gauss', 'spiral']):\n",
    "    data, labels = getattr(datasets, name)(200)\n",
    "    axis = plt.subplot(221+i)\n",
    "    axis.set_title(name)\n",
    "    axis.scatter(*zip(*data), c=labels, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# datasets.circle(), datasets.xor(), datasets.gauss(), datasets.spiral()\n",
    "data, labels = datasets.gauss(n=2000, noise=.5)\n",
    "(train_data, train_labels), (test_data, test_labels) = datasets.split((data, labels), 10)\n",
    "\n",
    "plt.figure('Choosen dataset')\n",
    "plt.scatter(*zip(*data), c=labels, cmap='bwr')\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn!\n",
    "In the next cell stuff gets interesting. We preimplemented a basic model which is sufficiently powerful for the gauss dataset. It consists of 2 neurons in the input layer to feed `x` and `y` values into, 4 neurons in the single hidden layer and again 2 neurons in the output layer for giving probabilities for each of our two classes.\n",
    "\n",
    "Instead of using buttons and dropdowns you will need to get your hands dirty here in order to being able to cope with more complex datasets! While we provide some guidance to mock the experience of the web application it will be also super useful to checkout the [TensorFlow API documentation](https://www.tensorflow.org/versions/r0.9/api_docs/index.html). Because the documentation does not give any theoretical background on all those different functions you might also want to regularily consultate Wikipedia.\n",
    "\n",
    "  * Adjust the hidden layer, play around with more or even fewer neurons.\n",
    "  * Add another hidden layer! You will need to adjust the matrix multiplications for its surrounding layers in order to integrate it.\n",
    "  * Change the activation functions. By default both, the hidden and the output layer, use the sigmoid activation function. Similar to the web application you can use `relu` or `tanh`. You might also consider trying to implement your own `linear` activation function -- TensorFlow does not provide one by itself.\n",
    "  * Take a look into different error measures and optimizers. This is nothing you can change in the playground web application but actually a powerful tool to tweak your networks performance through better training. Checkout the [optimizers chapter](https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#optimizers) in the documentation.\n",
    "  * Finally take a look at the actual training loop. Tweak the amount of iterations and change the batch size. You can actually also tweak the batch size in the web app, take a look at the bottom left. \n",
    "\n",
    "When you solved all the datasets (choose in the code cell above) you can also start adding noise to the data or take less data for training by adjusting the test/train split (again, both definable above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import permutation\n",
    "from playground import viz\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2], name='input_layer')\n",
    "\n",
    "W_h = tf.Variable(tf.random_normal([2,4]), name='hidden_weights')\n",
    "b_h = tf.Variable(tf.zeros([4]), name='hidden_bias')\n",
    "h = tf.nn.sigmoid(tf.matmul(x,W_h) + b_h, name='hidden_layer')\n",
    "\n",
    "W_y = tf.Variable(tf.random_normal([4,1]), name='output_weights')\n",
    "b_y = tf.Variable(tf.zeros([1]), name='output_bias')\n",
    "y = tf.nn.sigmoid(tf.matmul(h,W_y) + b_y, name='output_layer')\n",
    "\n",
    "t = tf.placeholder(tf.float32, shape=[None, 1], name='targets')\n",
    "\n",
    "error = tf.reduce_mean(tf.squared_difference(t,y), name='mean_squared_error')\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.03).minimize(error, name='gradient_descent')\n",
    "\n",
    "prediction_check = tf.equal(tf.round(y), t, name='prediction_check')\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32), name='accuracy_measure')\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    split = permutation(len(train_labels))\n",
    "    batch_labels = train_labels[split[:10]]\n",
    "    batch_data = train_data[split[:10]]\n",
    "    sess.run(optimizer, {x: batch_data, t: batch_labels})\n",
    "    if (epoch+1)%100 == 0:\n",
    "        perf = sess.run(accuracy, {x: test_data, t: test_labels})\n",
    "        predictions = sess.run(y, {x: data})\n",
    "        viz(data, predictions, (epoch+1)/epochs, perf, discretize=False)\n",
    "\n",
    "perf = sess.run(accuracy, {x: test_data, t: test_labels})\n",
    "predictions = sess.run(y, {x: data})\n",
    "viz(data, predictions, 1, perf, discretize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
